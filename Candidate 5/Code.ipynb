{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92282c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Analysis Insights:\n",
      "Dominant Emotion: neutral\n",
      "Emotional Range: 352.7061991888661\n",
      "Average Speech Speed: 2.8173408196402616\n",
      "Average Confidence: 0.5900939799406949\n",
      "Average Conciseness: 0.4136444577837692\n",
      "Average Enthusiasm: 0.3781099378076546\n",
      "Overall Sentiment: positive\n",
      "Gaze Percentage: 100.0\n",
      "Blink Rate: 0.25\n",
      "Average Eye Offset: 15.802625\n",
      "Total Duration: 72.0\n",
      "Average Distance: 0.0\n",
      "\n",
      "Plots 'emotion_timeline.png', 'speech_characteristics.png', and 'gaze_analysis.png' have been generated.\n",
      "Word Count: 205\n",
      "with: 5\n",
      "that: 4\n",
      "like: 4\n",
      "or: 4\n",
      "one: 3\n",
      "by: 3\n",
      "on: 3\n",
      "m: 2\n",
      "from: 2\n",
      "was: 2\n",
      "busy: 2\n",
      "doing: 2\n",
      "movie: 2\n",
      "it: 2\n",
      "neurodevelopmental: 2\n",
      "disorders: 2\n",
      "idea: 2\n",
      "what: 2\n",
      "works: 2\n",
      "for: 2\n",
      "students: 2\n",
      "help: 2\n",
      "these: 2\n",
      "hello: 1\n",
      "sakshi: 1\n",
      "come: 1\n",
      "mumbai: 1\n",
      "did: 1\n",
      "undergraduation: 1\n",
      "mass: 1\n",
      "media: 1\n",
      "specialization: 1\n",
      "advertising: 1\n",
      "completed: 1\n",
      "two: 1\n",
      "certification: 1\n",
      "courses: 1\n",
      "entrepreneurship: 1\n",
      "course: 1\n",
      "\n",
      "Expertise areas based on keyword frequency:\n",
      "Research: 3\n",
      "Medical Writing: 1\n",
      "Drug Safety: 1\n",
      "Management: 1\n",
      "Regulatory Affairs: 0\n",
      "Biotechnology: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "emotion_data = pd.read_csv('emotion_5.csv')\n",
    "transcript_data = pd.read_csv('transcriptscores_5.csv',delimiter = ',')\n",
    "transcript_text = pd.read_csv('transcripttext_5.txt', delimiter='\\t', header=None, names=['text'])\n",
    "gaze_data = pd.read_csv('gaze_5.csv')\n",
    "metadata = pd.read_csv('metadata_5.csv')\n",
    "# Load transcript text\n",
    "with open('transcripttext_5.txt', 'r') as file:\n",
    "    transcript_text1= file.read()\n",
    "\n",
    "# Combine transcript data\n",
    "transcript_data['text'] = transcript_text['text']\n",
    "\n",
    "def analyze_emotions(emotion_data):\n",
    "    emotion_summary = emotion_data[['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']].mean()\n",
    "    dominant_emotion = emotion_summary.idxmax()\n",
    "    \n",
    "    emotion_variance = emotion_data[['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']].var()\n",
    "    emotional_range = emotion_variance.sum()\n",
    "    \n",
    "    return emotion_summary, dominant_emotion, emotional_range\n",
    "\n",
    "def analyze_speech(transcript_data):\n",
    "    speech_speed = transcript_data['speech_speed'].mean()\n",
    "    confidence = transcript_data['confident'].mean()\n",
    "    conciseness = transcript_data['concise'].mean()\n",
    "    enthusiasm = transcript_data['enthusiastic'].mean()\n",
    "    \n",
    "    return speech_speed, confidence, conciseness, enthusiasm\n",
    "\n",
    "def analyze_sentiment(transcript_data):\n",
    "    sentiment_scores = transcript_data[['positive', 'negative', 'neutral']].mean()\n",
    "    overall_sentiment = sentiment_scores.idxmax()\n",
    "    \n",
    "    return sentiment_scores, overall_sentiment\n",
    "\n",
    "def analyze_gaze(gaze_data):\n",
    "    gaze_percentage = (gaze_data['gaze'] == 1).mean() * 100\n",
    "    blink_rate = gaze_data['blink'].mean()\n",
    "    avg_eye_offset = gaze_data['eye_offset'].abs().mean()\n",
    "    \n",
    "    return gaze_percentage, blink_rate, avg_eye_offset\n",
    "\n",
    "def analyze_metadata(metadata):\n",
    "    total_duration = metadata['elapsed_time'].max()\n",
    "    avg_distance = metadata['distance'].mean()\n",
    "    \n",
    "    return total_duration, avg_distance\n",
    "\n",
    "def generate_insights(emotion_data, transcript_data, gaze_data, metadata):\n",
    "    emotion_summary, dominant_emotion, emotional_range = analyze_emotions(emotion_data)\n",
    "    speech_speed, confidence, conciseness, enthusiasm = analyze_speech(transcript_data)\n",
    "    sentiment_scores, overall_sentiment = analyze_sentiment(transcript_data)\n",
    "    gaze_percentage, blink_rate, avg_eye_offset = analyze_gaze(gaze_data)\n",
    "    total_duration, avg_distance = analyze_metadata(metadata)\n",
    "    \n",
    "    insights = {\n",
    "        \"Dominant Emotion\": dominant_emotion,\n",
    "        \"Emotional Range\": emotional_range,\n",
    "        \"Average Speech Speed\": speech_speed,\n",
    "        \"Average Confidence\": confidence,\n",
    "        \"Average Conciseness\": conciseness,\n",
    "        \"Average Enthusiasm\": enthusiasm,\n",
    "        \"Overall Sentiment\": overall_sentiment,\n",
    "        \"Gaze Percentage\": gaze_percentage,\n",
    "        \"Blink Rate\": blink_rate,\n",
    "        \"Average Eye Offset\": avg_eye_offset,\n",
    "        \"Total Duration\": total_duration,\n",
    "        \"Average Distance\": avg_distance\n",
    "    }\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def plot_emotion_timeline(emotion_data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for emotion in ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']:\n",
    "        plt.plot(emotion_data['image_seq'], emotion_data[emotion], label=emotion)\n",
    "    \n",
    "    plt.title(\"Emotion Timeline\")\n",
    "    plt.xlabel(\"Video Timeline\")\n",
    "    plt.ylabel(\"Emotion Intensity\")\n",
    "    plt.legend()\n",
    "    plt.savefig('emotion_timeline.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_speech_characteristics(transcript_data):\n",
    "    characteristics = ['confident', 'concise', 'enthusiastic', 'speech_speed']\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(\"Speech Characteristics Over Time\")\n",
    "    \n",
    "    for i, characteristic in enumerate(characteristics):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "        ax.plot(transcript_data['start'], transcript_data[characteristic])\n",
    "        ax.set_title(characteristic.capitalize())\n",
    "        ax.set_xlabel(\"Time (seconds)\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('speech_characteristics.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_gaze_analysis(gaze_data):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(gaze_data['image_seq'], gaze_data['gaze'], label='Gaze')\n",
    "    plt.plot(gaze_data['image_seq'], gaze_data['eye_offset'], label='Eye Offset')\n",
    "    plt.title(\"Gaze and Eye Offset Over Time\")\n",
    "    plt.xlabel(\"Video Timeline\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.savefig('gaze_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "def get_word_frequency(text, top_n=50):\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "def identify_expertise_areas(text, keywords):\n",
    "    text_lower = text.lower()\n",
    "    expertise_scores = {}\n",
    "    for area, words in keywords.items():\n",
    "        score = sum(text_lower.count(word) for word in words)\n",
    "        expertise_scores[area] = score\n",
    "    return expertise_scores\n",
    "\n",
    "# Generate insights\n",
    "insights = generate_insights(emotion_data, transcript_data, gaze_data, metadata)\n",
    "\n",
    "# Generate plots\n",
    "plot_emotion_timeline(emotion_data)\n",
    "plot_speech_characteristics(transcript_data)\n",
    "plot_gaze_analysis(gaze_data)\n",
    "\n",
    "# Print insights\n",
    "print(\"Candidate Analysis Insights:\")\n",
    "for key, value in insights.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nPlots 'emotion_timeline.png', 'speech_characteristics.png', and 'gaze_analysis.png' have been generated.\")\n",
    "# Word frequency analysis\n",
    "\n",
    "# Analyze transcript content\n",
    "word_count = len(transcript_text1.split())\n",
    "print(f\"Word Count: {word_count}\")\n",
    "\n",
    "# List of stop words to remove\n",
    "stop_words = ['i', 'and', 'to', 'the', 'a', 'in', 'of', 'is', 'this', 'my','you','at','am','an','have','be']\n",
    "\n",
    "# Assuming `get_word_frequency` returns a list of tuples (word, count)\n",
    "word_freq = get_word_frequency(transcript_text['text'].str.cat(sep=' '))\n",
    "\n",
    "# Filter and print words that are not in stop_words\n",
    "for word, count in word_freq:\n",
    "    if word.lower() not in stop_words:  # Convert to lowercase for case-insensitive comparison\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "\n",
    "# Expertise areas based on word frequency\n",
    "expertise_keywords = {\n",
    "    \"Regulatory Affairs\": [\"regulatory\", \"affairs\", \"pharmaceutical\"],\n",
    "    \"Medical Writing\": [\"medical\", \"writer\", \"writing\"],\n",
    "    \"Drug Safety\": [\"drug\", \"safety\", \"risk\", \"management\"],\n",
    "    \"Research\": [\"research\", \"work\", \"patent\", \"publication\"],\n",
    "    \"Biotechnology\": [\"biotechnology\", \"tech\"],\n",
    "    \"Management\": [\"management\", \"postgraduate\",\"business\"]\n",
    "}\n",
    "\n",
    "expertise_areas = identify_expertise_areas(transcript_text['text'].str.cat(sep=' '), expertise_keywords)\n",
    "print(\"\\nExpertise areas based on keyword frequency:\")\n",
    "for area, score in sorted(expertise_areas.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{area}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050abed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
