# üé• Multi-Modal Video Interview Analytics for Recruitment Decisions

<div align="center">

*Revolutionizing recruitment with AI-powered multi-modal analysis*

[View Demo](#-demo) ‚Ä¢ [Report Bug](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues) ‚Ä¢ [Request Feature](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues)

</div>

---

## üìñ Table of Contents

- [About the Project](#-about-the-project)
- [Key Features](#-key-features)
- [Technology Stack](#-technology-stack)
- [Getting Started](#-getting-started)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Methodology](#-methodology)
- [Results & Performance](#-results--performance)
- [Sample Analysis](#-sample-analysis)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)
- [Acknowledgments](#-acknowledgments)

---

## üöÄ About the Project

**Multi-Modal Video Interview Analytics** is a cutting-edge AI-powered framework that transforms traditional recruitment processes by analyzing candidate introduction videos through multiple dimensions. This system provides comprehensive, data-driven insights to help HR professionals make more informed and objective hiring decisions.

### üéØ Problem Statement

Traditional video interviews often suffer from:
- **Subjective bias** in candidate evaluation
- **Inconsistent assessment** criteria across interviewers
- **Limited analytical depth** beyond surface-level observations
- **Time-intensive** manual review processes

### üí° Our Solution

Our multi-modal approach analyzes three critical dimensions:
- **üëÅÔ∏è Visual Analysis**: Facial expressions, emotions, and gaze patterns
- **üéµ Audio Analysis**: Speech prosody, confidence levels, and vocal characteristics
- **üìù Linguistic Analysis**: Content quality, sentiment, and communication skills

---

## ‚ú® Key Features

### üî¨ Advanced Analytics Modules

| Module | Description | Key Metrics |
|--------|-------------|-------------|
| **Facial Expression Analysis** | Real-time emotion detection using state-of-the-art CNNs | 7 emotion categories, valence & arousal |
| **Voice Prosody Extraction** | Speech pattern analysis with acoustic feature extraction | Pitch, energy, speech rate, spectral features |
| **Linguistic Processing** | NLP-powered transcript analysis | Sentiment, lexical richness, keyword frequency |
| **Gaze Tracking** | Eye movement and attention pattern analysis | Gaze percentage, blink rate, eye offset |
| **Multi-Modal Fusion** | Intelligent combination of all modalities | Confidence scores, ensemble predictions |

### üé® Visualization & Reporting

- **Interactive Dashboards**: Real-time emotion timelines and speech characteristics
- **Comprehensive Reports**: Automated candidate analysis documents
- **Performance Metrics**: ROC curves, confusion matrices, feature importance
- **Candidate Profiling**: Expertise area identification and skill mapping

### üîß Technical Capabilities

- **Scalable Architecture**: Process multiple candidates simultaneously
- **Robust Feature Engineering**: 50+ extracted features per modality
- **Model Interpretability**: SHAP values for explainable AI
- **Export Capabilities**: CSV, PDF, and interactive plot generation

---

## üõ†Ô∏è Technology Stack

<div align="center">

| Category | Technologies |
|----------|-------------|
| **Machine Learning** | ![Python](https://img.shields.io/badge/-Python-3776AB?style=flat&logo=python&logoColor=white) ![scikit-learn](https://img.shields.io/badge/-scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white) ![TensorFlow](https://img.shields.io/badge/-TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white) |
| **Computer Vision** | ![OpenCV](https://img.shields.io/badge/-OpenCV-5C3EE8?style=flat&logo=opencv&logoColor=white) ![FaceNet](https://img.shields.io/badge/-FaceNet-4285F4?style=flat) |
| **Audio Processing** | ![Librosa](https://img.shields.io/badge/-Librosa-FF6B6B?style=flat) ![PyAudio](https://img.shields.io/badge/-PyAudio-2E8B57?style=flat) |
| **NLP** | ![NLTK](https://img.shields.io/badge/-NLTK-00B4D8?style=flat) ![spaCy](https://img.shields.io/badge/-spaCy-09A3D5?style=flat&logo=spacy&logoColor=white) |
| **Data Analysis** | ![Pandas](https://img.shields.io/badge/-Pandas-150458?style=flat&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/-NumPy-013243?style=flat&logo=numpy&logoColor=white) |
| **Visualization** | ![Matplotlib](https://img.shields.io/badge/-Matplotlib-11557C?style=flat) ![Seaborn](https://img.shields.io/badge/-Seaborn-3776AB?style=flat) |

</div>

---

## üöÄ Getting Started

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-.git
   cd Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-
   ```

2. **Create and activate virtual environment**
   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate
   
   # macOS/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Verify installation**
   ```bash
   python -c "import cv2, librosa, pandas, sklearn; print('‚úÖ All dependencies installed successfully!')"
   ```

---

## üíª Usage

### Quick Start - Analyze a Single Candidate

```bash
# Navigate to candidate directory
cd "Candidate 1"

# Run the analysis notebook
jupyter notebook Code.ipynb
```

### Batch Processing Multiple Candidates

```python
import pandas as pd
from src.fusion_model import MultiModalAnalyzer

# Initialize analyzer
analyzer = MultiModalAnalyzer()

# Process all candidates
results = {}
for i in range(1, 11):
    candidate_path = f"Candidate {i}"
    results[f"candidate_{i}"] = analyzer.analyze_candidate(candidate_path)

# Generate comparative report
analyzer.generate_comparative_report(results)
```

### Individual Module Usage

```python
# Emotion Analysis
from src.face_emotion import EmotionAnalyzer
emotion_analyzer = EmotionAnalyzer()
emotions = emotion_analyzer.extract_emotions("video.mp4")

# Speech Analysis  
from src.audio_prosody import ProsodyAnalyzer
prosody_analyzer = ProsodyAnalyzer()
speech_features = prosody_analyzer.extract_features("audio.wav")

# Text Analysis
from src.nlp_features import LinguisticAnalyzer
nlp_analyzer = LinguisticAnalyzer()
text_features = nlp_analyzer.analyze_transcript("transcript.txt")
```

---

## üìÅ Project Structure

```
Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Candidate 1-10/          # Individual candidate analysis folders
‚îÇ   ‚îú‚îÄ‚îÄ üìì Code.ipynb          # Analysis notebook
‚îÇ   ‚îú‚îÄ‚îÄ üìä emotion_*.csv       # Emotion detection results
‚îÇ   ‚îú‚îÄ‚îÄ üëÅÔ∏è gaze_*.csv          # Gaze tracking data
‚îÇ   ‚îú‚îÄ‚îÄ üìà transcriptscores_*.csv # Speech analysis scores
‚îÇ   ‚îú‚îÄ‚îÄ üìù transcripttext_*.txt   # Interview transcripts
‚îÇ   ‚îú‚îÄ‚îÄ üìã metadata_*.csv        # Video metadata
‚îÇ   ‚îú‚îÄ‚îÄ üìä PLOTS/               # Generated visualizations
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Analysis Report.docx  # Comprehensive candidate report
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/                     # Source code modules
‚îÇ   ‚îú‚îÄ‚îÄ üé≠ face_emotion.py      # Facial expression detection
‚îÇ   ‚îú‚îÄ‚îÄ üéµ audio_prosody.py     # Voice feature extraction
‚îÇ   ‚îú‚îÄ‚îÄ üìù nlp_features.py      # Linguistic analysis
‚îÇ   ‚îú‚îÄ‚îÄ üîó fusion_model.py      # Multi-modal fusion architecture
‚îÇ   ‚îî‚îÄ‚îÄ üõ†Ô∏è utils.py             # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ üìÇ models/                  # Trained model checkpoints
‚îú‚îÄ‚îÄ üìÇ results/                 # Evaluation reports and visualizations
‚îú‚îÄ‚îÄ üìÇ notebooks/               # Jupyter notebooks for exploration
‚îú‚îÄ‚îÄ üìã requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ ‚öôÔ∏è setup.py                # Package installation script
‚îî‚îÄ‚îÄ üìñ README.md               # Project documentation
```

---

## üß† Methodology

### 1. Data Preprocessing Pipeline

```mermaid
graph LR
    A[Video Input] --> B[Frame Extraction]
    A --> C[Audio Extraction]
    A --> D[Transcript Generation]
    B --> E[Face Detection & Alignment]
    C --> F[Audio Segmentation]
    D --> G[Text Cleaning & Tokenization]
```

### 2. Feature Extraction

| Modality | Features Extracted | Count |
|----------|-------------------|-------|
| **Visual** | Emotion probabilities, facial action units, gaze patterns | 15+ |
| **Acoustic** | MFCCs, pitch contours, energy dynamics, prosody | 20+ |
| **Textual** | TF-IDF vectors, sentiment scores, POS frequencies | 15+ |

### 3. Model Architecture

- **Uni-modal Classifiers**: Individual models for each modality
- **Late Fusion**: Weighted combination of modal predictions  
- **Ensemble Methods**: Random Forest, XGBoost, Neural Networks
- **Interpretability**: SHAP values for feature importance

### 4. Evaluation Metrics

- **Classification**: Accuracy, Precision, Recall, F1-Score
- **Ranking**: AUC-ROC, AUC-PR
- **Reliability**: Cross-validation, confidence intervals

---

## üìä Results & Performance

### Overall Model Performance

| Metric | Score | Benchmark |
|--------|-------|-----------|
| **ROC AUC** | 0.92 | ‚≠ê Excellent |
| **F1-Score** | 0.88 | ‚≠ê Excellent |
| **Accuracy** | 0.90 | ‚≠ê Excellent |
| **Precision** | 0.89 | ‚≠ê Excellent |
| **Recall** | 0.87 | ‚≠ê Very Good |

### Feature Importance Ranking

1. **Speech Confidence** (0.23) - Most predictive feature
2. **Dominant Emotion** (0.19) - Emotional stability indicator  
3. **Gaze Percentage** (0.16) - Attention and engagement
4. **Linguistic Complexity** (0.14) - Communication skills
5. **Voice Prosody** (0.12) - Vocal characteristics

---

## üîç Sample Analysis

### Candidate Profile Example

**Candidate 1 - Analysis Summary**

| Dimension | Score | Insights |
|-----------|-------|----------|
| **Emotional Stability** | 85% | Predominantly neutral expression with controlled emotional range |
| **Communication Skills** | 73% | Confident speech delivery with good conciseness |
| **Engagement Level** | 63% | Moderate eye contact, room for improvement |
| **Content Quality** | 78% | Strong technical vocabulary, research-oriented background |
| **Overall Suitability** | 75% | **Recommended** for technical roles |

**Key Strengths:**
- üéØ Strong expertise in biotechnology and research (7 keyword matches)
- üí¨ Confident communication style (0.73 average confidence score)
- üòä Positive sentiment throughout interview

**Areas for Development:**
- üëÅÔ∏è Improve eye contact consistency (62.5% gaze percentage)
- üé§ Enhance speech enthusiasm (0.47 average score)

---

## ü§ù Contributing

We welcome contributions from the community! Here's how you can help:

### Ways to Contribute

- üêõ **Bug Reports**: Found an issue? [Report it here](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues)
- üí° **Feature Requests**: Have an idea? [Suggest it here](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues)
- üîß **Code Contributions**: Submit pull requests with improvements
- üìñ **Documentation**: Help improve our documentation

### Development Setup

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Standards

- Follow PEP 8 style guidelines
- Add docstrings to all functions
- Include unit tests for new features
- Update documentation as needed

---

## üîÆ Future Enhancements

### Planned Features

- [ ] **Real-time Analysis**: Live interview processing capabilities
- [ ] **Web Dashboard**: Interactive web interface for HR teams
- [ ] **API Integration**: RESTful API for system integration
- [ ] **Mobile App**: Companion mobile application
- [ ] **Advanced Models**: Transformer-based architectures
- [ ] **Bias Detection**: Fairness and bias monitoring tools

### Research Directions

- [ ] **Multi-language Support**: Extend analysis to multiple languages
- [ ] **Cultural Adaptation**: Culture-specific emotion recognition
- [ ] **Longitudinal Analysis**: Track candidate development over time
- [ ] **Industry Specialization**: Domain-specific evaluation models

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Bhavishya Gupta

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

---

## üë§ Contact & Support

<div align="center">

**Bhavishya Gupta**

[![GitHub](https://img.shields.io/badge/-GitHub-181717?style=flat&logo=github)](https://github.com/Bhavishya-Gupta)
[![LinkedIn](https://img.shields.io/badge/-LinkedIn-0A66C2?style=flat&logo=linkedin)](https://linkedin.com/in/bhavishya-gupta)
[![Email](https://img.shields.io/badge/-Email-D14836?style=flat&logo=gmail&logoColor=white)](mailto:bhavishya.gupta@example.com)

*Let's connect and discuss how AI can transform recruitment!*

</div>

### Project Support

- üìß **General Questions**: [Create an issue](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues)
- üí¨ **Discussions**: [Join our discussions](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/discussions)
- üÜò **Bug Reports**: [Report bugs here](https://github.com/Bhavishya-Gupta/Multi-Modal-Video-Interview-Analytics-for-Recruitment-Decisions-/issues/new?template=bug_report.md)

---

## üôè Acknowledgments

Special thanks to:

- **OpenCV Community** for computer vision tools
- **Librosa Team** for audio processing capabilities  
- **scikit-learn Contributors** for machine learning frameworks
- **Open Source Community** for inspiration and collaboration
- **Research Community** for foundational work in multi-modal analysis

---

<div align="center">

**‚≠ê Star this repository if you found it helpful!**

*Making recruitment fairer, faster, and more effective with AI*

[![Made with ‚ù§Ô∏è](https://img.shields.io/badge/Made%20with-‚ù§Ô∏è-red)](https://github.com/Bhavishya-Gupta)
[![Python](https://img.shields.io/badge/Made%20with-Python-blue)](https://python.org)

</div>

---

*Last updated: September 2024*
